---
title: Activation Function
date: 2019-07-09 18:11:40
tags:
keywords:
mathjax: true
categories: 
- [Artificial Intelligence, Deep Learning]
description: æ¿€æ´»å‡½æ•°çš„å½¢å¼ï¼Œæ¢¯åº¦ï¼Œæ€§è´¨ã€‚
---

## Neuron

![](resources/1.png)
å¦‚å›¾æ˜¯ç¥ç»ç½‘ç»œä¸­ä¸€ä¸ªå…¸å‹çš„ç¥ç»å…ƒè®¾è®¡ï¼Œå®ƒå®Œå…¨ä»¿ç…§äººç±»å¤§è„‘ä¸­ç¥ç»å…ƒä¹‹é—´ä¼ é€’æ•°æ®çš„æ¨¡å¼è®¾è®¡ã€‚å¤§è„‘ä¸­ï¼Œç¥ç»å…ƒé€šè¿‡è‹¥å¹²æ ‘çªï¼ˆdendriteï¼‰çš„çªè§¦ï¼ˆsynapseï¼‰ï¼Œæ¥å—å…¶ä»–ç¥ç»å…ƒçš„è½´çªï¼ˆaxonï¼‰æˆ–æ ‘çªä¼ é€’æ¥çš„æ¶ˆæ¯ï¼Œè€Œåç»è¿‡å¤„ç†å†ç”±è½´çªè¾“å‡ºã€‚


## å‚æ•°æ›´æ–°æ–¹å‘

æ·±åº¦å­¦ä¹ ä¸€èˆ¬çš„å­¦ä¹ æ–¹æ³•æ˜¯åå‘ä¼ æ’­ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæ±‚è§£å…¨å±€æŸå¤±å‡½æ•°L(x)å¯¹äºæŸä¸€å‚æ•°wçš„åå¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ï¼›è€Œåè¾…ä»¥å­¦ä¹ ç‡\eta,å‘æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°w

$$ w \leftarrow w - \eta\cdot\frac{\partial L}{\partial w} $$

è€ƒè™‘å­¦ä¹ ç‡$\eta$æ˜¯å…¨å±€è®¾ç½®çš„è¶…å‚æ•°ï¼Œå‚æ•°æ›´æ–°çš„æ ¸å¿ƒæ­¥éª¤å³æ˜¯è®¡ç®—$\frac{\partial L}{\partial w}$ å†è€ƒè™‘åˆ°å¯¹äºæŸä¸ªç¥ç»å…ƒæ¥è¯´ï¼Œå…¶è¾“å…¥ä¸è¾“å‡ºçš„å…³ç³»æ˜¯
$$f(x;w,b) = f(\sum_{i}w_i x_i + b)$$
æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œå¯¹äºå‚æ•°w_iæ¥è¯´
$$\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial f}\frac{\partial f}{\partial w_i} = x_i\cdot\frac{\partial L}{\partial f} $$

å› æ­¤ï¼Œå‚æ•°çš„æ›´æ–°æ­¥éª¤å˜ä¸º

$$ w \leftarrow w - \eta x_i\cdot\frac{\partial L}{\partial f} $$

ç”±äº$w_i$æ˜¯ä¸Šä¸€è½®è¿­ä»£çš„ç»“æœï¼Œæ­¤å¤„å¯è§†ä¸ºå¸¸æ•°ï¼Œè€Œ$\eta$æ˜¯æ¨¡å‹è¶…å‚æ•°ï¼Œå‚æ•°$w_i$çš„æ›´æ–°æ–¹å‘å®é™…ä¸Šç”±$x_i\cdot\frac{\partial L}{\partial f}$å†³å®šã€‚åˆè€ƒè™‘åˆ° $\frac{\partial L}{\partial f}$å¯¹äºæ‰€æœ‰çš„$w_i$æ¥è¯´æ˜¯å¸¸æ•°ï¼Œå› æ­¤å„ä¸ª$w_i$æ›´æ–°æ–¹å‘ä¹‹é—´çš„å·®å¼‚ï¼Œå®Œå…¨ç”±å¯¹åº”çš„è¾“å…¥å€¼$x_i$ çš„ç¬¦å·å†³å®šã€‚




## ä»¥é›¶ä¸ºä¸­å¿ƒçš„å½±å“

è‡³æ­¤ï¼Œä¸ºäº†æè¿°æ–¹ä¾¿ï¼Œæˆ‘ä»¬ä»¥äºŒç»´çš„æƒ…å†µä¸ºä¾‹ã€‚äº¦å³ï¼Œç¥ç»å…ƒæè¿°ä¸º

$$f(x;w_1,w_2,b) = f(w_0x_0 + w_1x_1 + b)$$

å‡è®¾å¯¹äºå‚æ•° $w_0$,$w_1$çš„æœ€ä¼˜è§£$w_0^{\prime}$,$w_1^{\prime}$

\begin{equation}
\left\{ 
\begin{array}{lr}

& w_0 < w_0^{\prime} \\
& w_1 > w_1^{\prime}

\end{array}
\right.
\end{equation}

æˆ‘ä»¬å¸Œæœ›$w_0$é€‚å½“å¢å¤§ï¼Œ$w_1$é€‚å½“å‡å°‘ï¼Œè¿™é‡Œå¿…ç„¶è¦æ±‚$x_0$ å’Œ $x_1$ ç¬¦å·ç›¸åã€‚
ä½†åœ¨ Sigmoid å‡½æ•°ä¸­ï¼Œè¾“å‡ºå€¼æ’ä¸ºæ­£ã€‚è¿™ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä¸Šä¸€çº§ç¥ç»å…ƒé‡‡ç”¨ Sigmoid å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œ**é‚£ä¹ˆæˆ‘ä»¬æ— æ³•åšåˆ°$x_0$å’Œ$x_1$ç¬¦å·ç›¸å**ã€‚æ­¤æ—¶ï¼Œæ¨¡å‹ä¸ºäº†æ”¶æ•›ï¼Œä¸å¾—ä¸å‘é€†é£å‰è¡Œçš„é£åŠ©åŠ›å¸†èˆ¹ä¸€æ ·ï¼Œèµ° Z å­—å½¢é€¼è¿‘æœ€ä¼˜è§£ã€‚

![](resources/3.png)



## Sigmid 

### å‡½æ•°å½¢å¼
$$\sigma(x;a) = \frac{1}{1+e^{-ax}}$$

### å¯¼æ•°

$$\sigma^{\prime} = \sigma(x)(1-\sigma(x))$$

### æ€§è´¨
- Sigmoid neurons can saturate and lead to vanishing gradients.
- Not zero-centered.
- $e^{x}$is computationally expensive.


## Tanh

### å‡½æ•°å½¢å¼
$$tanh(x) = 2\sigma(2x) - 1 = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

### å¯¼æ•°

$$\tanh^{\prime} = 1-\tanh^{2}(x)$$

### æ€§è´¨
- Zero-centered.
- $e^{x}$is computationally expensive.
- tanh can saturate and lead to vanishing gradients.



## Relu

### å‡½æ•°å½¢å¼
$$f(x) = max(0,x)$$


### æ€§è´¨
- Fast to compute.
- Gradients do not vanish for ğ‘¥ > 0.
- Provides faster convergence in practice!
- Not zero-centered.
- Can die: if not activated, never updates!


## Relu

### å‡½æ•°å½¢å¼
$$f(x) = max(ax,x)$$

### æ€§è´¨
- Will not die
- a != 1
- zero-centered



